# -*- coding: utf-8 -*-
"""01_classify.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lD78msI3WoDmFuN4DTYJ-aW5oNps3Dyc

# <font color='green'>Sentiment Analysis for Financial Microblogs (Stocktwits and Twitter) </font>

1. **Feature Engineering: experiment with different types of features**
>1. TFIDF NGrams:
>>1. Unigrams
>>2. Unigrams + Bigrams
>>3. Unigrams + Bigrams + Trigrams (Best Results)
2. **Experiment with Different Classifiers**
>1. Naive Bayes
>2. Maximum Entropy
>3. SVM (Best Results)
>4. LM Corpus
3. **Output Most Informative Features**

### <font color=blue> Imported Libraries </font>
* Similar to Milestone (1)
"""

# Python standard libraries
import os, sys, re, csv, string, codecs

# Random number generation, and list shuffling
from random import shuffle

# library to connect with postgres. (without flask)
import psycopg2 as p
# For linear algebra
import numpy as np

# For graph plotting
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches

# Sklearn for machine learning modules
from sklearn import metrics
from sklearn.svm import LinearSVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.decomposition import TruncatedSVD
from sklearn.model_selection import ShuffleSplit
from sklearn.model_selection import learning_curve
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer

# Configurations
training_data_set_size = "400K"

# Input data files
processed_training_data_file_name = "data/output/processed_training_data_" + training_data_set_size + ".txt"
lm_positive_file_name = "data/LM_Positive.txt"
lm_negative_file_name = "data/LM_Negative.txt"

# Output data files
SVM_positive_file_name = "data/output/SVM_Positive.txt"
SVM_negative_file_name = "data/output/SVM_Negative.txt"


# Read processed training data saved by 0_process.ipynb
def read_processed_training_data(processed_training_data_file_name):
    all_text = []
    all_labels = []
    training_data_processed = []
    with codecs.open(processed_training_data_file_name, "r", encoding='utf-8',
                     errors='ignore') as processed_training_data_file:
        for processed_training_entry in processed_training_data_file:
            processed_training_entry = processed_training_entry.rstrip().split("\t")
            processed_text = processed_training_entry[1]
            label = processed_training_entry[2]

            all_text.append(processed_text)
            all_labels.append(label)
            training_data_processed.append([processed_text, label])

    return all_text, all_labels, training_data_processed


# Split processed training dataset to train / validation sets with ratios 90% to 10%
def get_train_validation_sets(training_data_processed, validation_percent):
    train, validation = train_test_split(training_data_processed, test_size=validation_percent, random_state=32)

    ###############################################################################################################
    validation = [];
    temp = [];
    con = p.connect(
        "dbname='tweets' user='postgres' password='Ec3dt8Xiw3IjJ9tIYFIz' host='localhost'")  # connecting to Database
    cur = con.cursor()
    cur.execute("select tweet_id, text from fintweet.tweet limit 100000 offset " + str(
        processed - 100000))  # query executed all tweet_id and its text extracted from database.
    rows = cur.fetchall()
    t_id = {};
    for r in rows:  # Preparing the new Validation set (from database)
        t_id[r[0]] = r[1];
        temp.append(r[1]);
        temp.append("ABC")
        validation.append(temp);  # it is of the form list of lists
        temp = [];

    ################################################################################################################

    # Extract processed text and labels from train / validation tests
    train_text = [processed_string for [processed_string, label] in train]
    train_labels = [label for [processed_string, label] in train]

    validation_text = [processed_string for [processed_string, label] in validation]
    validation_labels = [label for [processed_string, label] in validation]

    return train_text, train_labels, validation_text, validation_labels


# Extract unigram TFIDF features for train / validation sets
def tfidf_feature_extractor(all_text, train_text, validation_text, ngram_count):
    # Count ngrams for the training set, and apply features to count ngrams for validation set
    count_vectorizer = CountVectorizer(binary=True, min_df=2, ngram_range=(1, ngram_count))
    all_counts = count_vectorizer.fit_transform(all_text)
    train_counts = count_vectorizer.transform(train_text)
    validation_counts = count_vectorizer.transform(validation_text)

    # Generate TFIDF model based on the training set, and apply it to the validation set
    tfidf_transformer = TfidfTransformer()
    all_tfidf = tfidf_transformer.fit_transform(all_counts)
    train_tfidf = tfidf_transformer.transform(train_counts)
    validation_tfidf = tfidf_transformer.transform(validation_counts)

    return count_vectorizer, all_tfidf, train_tfidf, validation_tfidf


# Print evaluation metric, accuracy, precision, recall, f1-score
# For more information, check: https://blogs.msdn.microsoft.com/andreasderuiter/2015/02/09/performance-measures-in-azure-ml-accuracy-precision-recall-and-f1-score/
def print_evaluation_metrics(validation_labels, predicted, title):
    print("=====================================================")
    print("Classifier Model: " + title)

    print("=====================================================")
    print("Accuracy on validation set: {:10.2f}".format(metrics.accuracy_score(validation_labels, predicted) \
                                                        * float(100)) + "%")
    print("=====================================================")

    class_labels = ["Bullish", "Bearish"]
    print(metrics.classification_report(validation_labels, predicted, target_names=class_labels))
    print("=====================================================")


# Calculate True / False Positive Rates, and thresholds for the trained model on the validation set
# Goal: plot ROC curve
def get_true_false_positive_rates(ground_truth_labels, predicted_labels):
    ground_truth_labels_binary = [1 if label == "Bullish" else 0 for label in ground_truth_labels]
    predicted_labels_binary = [1 if label == "Bullish" else 0 for label in predicted_labels]

    return metrics.roc_curve(ground_truth_labels_binary, predicted_labels_binary)


# Plot ROC Curve
def plot_roc_curve(false_positive_rate, true_positive_rate, title):
    # Clear plot
    plt.clf()

    # Format plot
    fig, ax = plt.subplots()

    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.01])

    ax.tick_params(axis='x', labelsize=15)
    ax.tick_params(axis='y', labelsize=15)

    plt.title(title, fontsize=30)
    ax.set_ylabel("True Positive Rate", fontsize=30)
    ax.set_xlabel("False Positive Rate", fontsize=30)

    # Add data
    plt.plot([0, 1], [0, 1], color='lightgrey', linestyle='--')
    plt.plot(false_positive_rate, true_positive_rate, color="red")

    # Show plot
    plt.show()


# Plot Learning Curve for cross-validation dataset
def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None):
    # Clear plot
    plt.clf()

    # Initialize plot
    fig, ax = plt.subplots()

    # Set plot labels
    plt.title(title, fontsize=30)
    plt.ylabel("Accuracy", fontsize=30)
    plt.xlabel("Training examples", fontsize=30)

    # Set X, Y axes ticks sizes
    ax.tick_params(axis='x', labelsize=15)
    ax.tick_params(axis='y', labelsize=15)

    # Set Y axis limit
    if ylim is not None:
        plt.ylim(*ylim)

    # Process Data
    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv)
    train_scores_mean = np.mean(train_scores, axis=1)
    train_scores_std = np.std(train_scores, axis=1)
    test_scores_mean = np.mean(test_scores, axis=1)
    test_scores_std = np.std(test_scores, axis=1)

    # Add Data to plot
    plt.grid()
    plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std,
                     alpha=0.1, color="r")
    plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1,
                     color="g")
    plt.plot(train_sizes, train_scores_mean, 'o-', color="r", label="Training Accuracy")
    plt.plot(train_sizes, test_scores_mean, 'o-', color="g", label="Cross-validation Accuracy")

    # Configure legend position and show plot
    plt.legend(loc="best", fontsize=20)
    plt.show()


# Plot predicted vs. ground truth labels for validation / test sets
def plot_predicted_ground_trurth(data_pca, title, labels, predicted_labels):
    # Prepare Data
    labels_colors = ["green" if label == "Bullish" else "red" for label in labels]
    predicted_labels_colors = ["green" if label == "Bullish" else "red" \
                               for label in predicted_labels]

    # Clear plot
    plt.clf()

    # Initialize plot
    fig, ax = plt.subplots(1, 2)

    # Set plot labels
    fig.suptitle(title, fontsize=30)
    fig.subplots_adjust(top=0.85)

    # Set X, Y axes ticks sizes
    ax[0].tick_params(axis='x', labelsize=15)
    ax[0].tick_params(axis='y', labelsize=15)
    ax[1].tick_params(axis='x', labelsize=15)
    ax[1].tick_params(axis='y', labelsize=15)

    # Add Data to plot
    ax[0].scatter(data_pca[:, 0], data_pca[:, 1], c=predicted_labels_colors)
    ax[0].set_title('Predicted Labels', fontsize=20)
    ax[1].scatter(data_pca[:, 0], data_pca[:, 1], c=labels_colors)
    ax[1].set_title('Ground Truth Labels', fontsize=20)

    # Configure legend
    green_patch = mpatches.Patch(color="green", label="Bullish")
    red_patch = mpatches.Patch(color="red", label='Bearish')
    plt.legend(loc="best", fontsize=20, handles=[red_patch, green_patch])

    # Show plot
    plt.show()


con = p.connect(
    "dbname='tweets' user='postgres' password='Ec3dt8Xiw3IjJ9tIYFIz' host='localhost'")  # connecting to Database

cur = con.cursor()
cur.execute("select count(tweet_id) from fintweet.tweet")  # query executed
rows = cur.fetchall()  # results stored in rows
total_tweet_count = rows[0][0]
print("Total Tweets = " + str(total_tweet_count))

processed = 0
while (processed < total_tweet_count):
    print("Processed " + str(processed) + "/" + str(total_tweet_count) + " tweets")
    processed = processed + 100000

    all_text, all_labels, training_data_processed = read_processed_training_data(processed_training_data_file_name)

    train_text, train_labels, validation_text, validation_labels = get_train_validation_sets(training_data_processed,
                                                                                             0.1)

    # NGrams range, default=3, consider Unigrams, Bigrams, Trigrams
    ngram_count = 3

    count_vectorizer, all_tfidf, train_tfidf, validation_tfidf = tfidf_feature_extractor(all_text, train_text,
                                                                                         validation_text, ngram_count)

    # Set features used for training the classifier (default: TFIDF)
    training_features = train_tfidf
    validation_features = validation_tfidf
    all_features = all_tfidf

    # Update figure sizes
    plt.rcParams["figure.figsize"] = [15, 12]

    # nb_model = MultinomialNB().fit(training_features, train_labels)
    # nb_predicted = nb_model.predict(validation_features)
    # max_ent_model = LogisticRegression().fit(training_features, train_labels)
    # max_ent_predicted = max_ent_model.predict(validation_features)
    svm_model = LinearSVC(loss='hinge').fit(training_features, train_labels)
    svm_predicted = svm_model.predict(validation_features)

    tweets = {}  # EMPTY DICTIONARY
    con = p.connect("dbname='tweets' user='postgres' password='Ec3dt8Xiw3IjJ9tIYFIz' host='localhost'")
    cur = con.cursor()
    cur.execute(
        "select tweet_id, text from fintweet.tweet limit 100000 offset " + str(processed - 100000))  # query executed
    rows = cur.fetchall()  # results stored in rows
    i = 0;
    for entry in rows:  # iterating selected data to set tweet_id corresponing to sentiment
        tweets[entry[0]] = svm_predicted[i];
        i = i + 1;
    for key in tweets:  # inserting tweet_id and its coressponding sentiment

        # query = "INSERT INTO fintweet.tweet_sentiment(tweet_id,sentiment) VALUES(" + str(key) + " , '" + tweets[key]+"')"
        query = "UPDATE fintweet.tweet_sentiment SET sentiment = '" + tweets[key] + "' WHERE tweet_id = " + str(
            key) + ";"
        # print(query)
        cur.execute(
            query);  # query executed but in "INSERT" case we have to comit changes to let it update the darabase

    con.commit()  # commiting changes to database.

exit();
########################################################################################################################


"""### <font color=blue> LM Corpus </font>
* More info: Loughran-McDonald Sentiment Word Lists
> - https://sraf.nd.edu/textual-analysis/resources/
"""


# Load LM Corpus (Positive / Negative)
def load_lm_corpus():
    with codecs.open(lm_positive_file_name, "r", encoding='utf-8', errors='ignore') as lm_positive_file:
        LM_Positive = lm_positive_file.read().splitlines()

    with codecs.open(lm_negative_file_name, "r", encoding='utf-8', errors='ignore') as lm_negative_file:
        LM_Negative = lm_negative_file.read().splitlines()

    LM_Positive = set([token.lower() for token in LM_Positive])
    LM_Negative = set([token.lower() for token in LM_Negative])

    return LM_Positive, LM_Negative


LM_Positive, LM_Negative = load_lm_corpus()

"""* Classify validation / test set using keywords from the LM Corpus
* If microblog (Stocktwit / Tweet) contains more positive keywords than negative, classify "Bullish"
* Otherwise, classify "Bearish"
* If microblog contains equal number of positive and negative keywords, or none of them, classify as "Bullish" -> defualt
* Since our current training / testing datasets contains only Bullish / Bearish labels, we only consider the positive/negative LM Corpus lists
"""


# Classify validation / test set using keywords from the LM corpus
def lm_corpus_classify(test_set):
    predicted = []
    for entry in test_set:
        entry_tokenized = set(entry.split(" "))

        bullish_count = len(entry_tokenized.intersection(LM_Positive))
        bearish_count = len(entry_tokenized.intersection(LM_Negative))

        # If positive >=0 Bullish, if negative < 0 Bearish
        # If no intersection with LM corpus, assume positive
        sentiment = bullish_count - bearish_count
        if sentiment >= 0:
            predicted.append('Bullish')
        else:
            predicted.append('Bearish')

    return predicted


lm_corpus_predicted = lm_corpus_classify(validation_text)

"""## <font color=green> Evaluate Classifiers (Validation) </font>

### <font color=blue> Accuracy, Precision, Recall, F1-Score </font>
"""

print_evaluation_metrics(validation_labels, nb_predicted, "Naive Bayes")

# print_evaluation_metrics(validation_labels, max_ent_predicted, "MaxEnt (Logistic Regression)")

# print_evaluation_metrics(validation_labels, svm_predicted, "SVM")

# print_evaluation_metrics(validation_labels, lm_corpus_predicted, "LM Corpus")

"""### <font color=blue> ROC Curves </font>"""

false_positive_rate, true_positive_rate, thresholds = get_true_false_positive_rates(validation_labels, nb_predicted)
# plot_roc_curve(false_positive_rate, true_positive_rate, "ROC (Naive Bayes)")

false_positive_rate, true_positive_rate, thresholds = get_true_false_positive_rates(validation_labels,
                                                                                    max_ent_predicted)
# plot_roc_curve(false_positive_rate, true_positive_rate, "ROC (MaxEnt \"Logistic Regression\")")

false_positive_rate, true_positive_rate, thresholds = get_true_false_positive_rates(validation_labels, svm_predicted)
# plot_roc_curve(false_positive_rate, true_positive_rate, "ROC (SVM)")

false_positive_rate, true_positive_rate, thresholds = get_true_false_positive_rates(validation_labels,
                                                                                    lm_corpus_predicted)
# plot_roc_curve(false_positive_rate, true_positive_rate, "ROC (LM Corpus)")

"""### <font color=blue> Cross Validation Learning Curve </font>
* Train a 10 fold cross validation classifier and plot the learning curve
"""

# Train a 10 fold cross validation classifier and plot the learning curve
cross_validation = ShuffleSplit(n_splits=10, test_size=0.1, random_state=32)

# plot_learning_curve(MultinomialNB(), "Learning Curves (Naive Bayes)", all_features, all_labels, cv=cross_validation)

# plot_learning_curve(LogisticRegression(), "Learning Curves (MaxEnt \"Logistic Regression\")", all_features, all_labels, cv=cross_validation)

# plot_learning_curve(LinearSVC(loss = 'hinge'), "Learning Curves (SVM)", all_features, all_labels, cv=cross_validation)

"""### <font color=blue> Predicted vs. Ground Truth Labels Plots </font>"""

data_pca = TruncatedSVD(n_components=2).fit_transform(training_features)
# plot_predicted_ground_trurth(data_pca, "Predicted vs. Ground Truth Labels (Naive Bayes)", \
# validation_labels, nb_predicted)

# plot_predicted_ground_trurth(data_pca, "Predicted vs. Ground Truth Labels (MaxEnt \"Logistic Regression\"))", \
# validation_labels, max_ent_predicted)

# plot_predicted_ground_trurth(data_pca, "Predicted vs. Ground Truth Labels (SVM)", \
#                  validation_labels, svm_predicted)

# plot_predicted_ground_trurth(data_pca, "Predicted vs. Ground Truth Labels (LM Corpus)", \
#                 validation_labels, lm_corpus_predicted)

"""***

## <font color='magenta'> 3. Output Most Informative Features </font>

### <font color=blue> SVM (Best Performing) Most Informative Features </font>
"""


# Get the top 10K most informative features for both classes Bullish / Bearish
def get_most_informative_features(vectorizer, estimator, n=10000):
    feature_names = vectorizer.get_feature_names()
    coefs_with_fns = sorted(zip(estimator.coef_[0], feature_names))
    most_informative_features = list(zip(coefs_with_fns[:n], coefs_with_fns[:-(n + 1):-1]))

    return most_informative_features


# Write the top 10K most informative features to disk with their scores
def write_most_informative_features(most_informative_features):
    with codecs.open(SVM_positive_file_name, "w", encoding='utf-8', errors='ignore') as SVM_positive_file, \
            codecs.open(SVM_negative_file_name, "w", encoding='utf-8', errors='ignore') as SVM_negative_file:
        for (coef_1, fn_1), (coef_2, fn_2) in most_informative_features:
            SVM_negative_file.write("{:.4f}".format(coef_1) + "\t" + fn_1 + "\n")
            SVM_positive_file.write("{:.4f}".format(coef_2) + "\t" + fn_2 + "\n")


# Print top #50 most infomative features for each class with their weights
def display_most_informative_features(most_informative_features, n=50):
    print("\tBearish\t\t\t\tBullish")
    print("=================================================================")
    print("\tScore\tFeature\t\t\tScore\tFeature")
    print("=================================================================")
    for (coef_1, fn_1), (coef_2, fn_2) in most_informative_features[:n]:
        print("\t%.4f\t%-15s\t\t%.4f\t%-15s" % (coef_1, fn_1, coef_2, fn_2))


most_informative_features = get_most_informative_features(count_vectorizer, svm_model)
write_most_informative_features(most_informative_features)
display_most_informative_features(most_informative_features)

"""***"""
